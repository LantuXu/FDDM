model:
  params:
    # Training parameters
    time_steps: 1000  # Time step size, starting from 1 to 1000
    linear_start: 1e-4  # The starting point for linear beta
    linear_end: 2e-2  # The end point of linear beta
    base_learning_rate: 5.0e-5
    batch_size: 32
    epoch: 1000
    image_directory: '/root/data/train/flower/img/jpg'
    text_directory: '/root/data/train/flower/text_flower'
    img_h: 256  # Height of the image
    img_w: 256  # Width of the image
    vlb_Switch: True  # Whether or not to reconstruct the ELBO loss
    v_posterior: 0  # Hyperparameter, used to interpolate the optimal variance with the full forward variance
    original_elbo_weight: 0

    snap: 10  # Every generations retain model parameters
    out_path: "output"  # Path to the output image
    out_num: 10  # The number of output images

    device: "cuda"
    pretrain_device: "cuda"  # Location of the pre-trained model

    z_coef: 1          # Frequency domain information coefficient, range from 0 to 1
    z_in: 6             # Number of channels for additional embedded information
    z_dim: 64           # The size of the additional embedding information vector
    z_layer: 6          # Number of convolutional layers for extra information encoding
    mask_type: 1        # Decide on the mask type: 1 is high-pass filtering, keeping high frequencies, 0 is no filtering, and -1 is low-pass filtering, keeping low frequencies
    mask_radius: 50     # Filter mask radius, depending on image size
    gaussian_mask: True    # Whether to use Gaussian mask or not
    gen_txt_path: "flower_gen.txt"    # The location of the txt file used for generation

    # DDIM parameter
    DDIM_Switch: True
    DDIM_timesteps: 200  # The time step used for DDIM sampling, is used to interpolate in the time step of DDPM
    DDIM_eta: 0  # It is used to adjust the certainty and uncertainty of the generation
    # VAE parameters
    VAE_Switch: True
    VAE_path: 'stabilityai/sd-vae-ft-mse'
    VAE_config_path: ''

    # CLIP parameter
    CLIP_Switch: True  # Only enabled, non-text training interface is not set
    CLIP_path: 'openai/clip-vit-base-patch32'

    # Unet parameters
    Unet_config:
      params:
        use_checkpoint: True
        use_fp16: True
        in_channels: 4
        out_channels: 4
        model_channels: 256
        attention_resolutions: [ 4, 2, 1 ]
        num_res_blocks: 2
        channel_mult: [ 1, 2, 4, 4 ]
        num_head_channels: 64 # need to fix for flash-attn
        use_spatial_transformer: True
        use_linear_in_transformer: True
        transformer_depth: 1
        context_dim: 768  # Be consistent with the last dimension after text encoding
        # Hyper
        z_coef: 1  # The hypernetwork layer is not enabled by default
        z_in: 6             # Number of channels for additional embedded information
        z_dim: 64           # The size of the additional embedding information vector
        z_layer: 6          # Number of convolutional layers for extra information encoding
#        legacy: False


